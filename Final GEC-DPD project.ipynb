{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Global Electronics and Communications EDA***\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the process of analyzing data using statistical and graphical methods to understand its structure, uncover patterns, and identify anomalies before modeling.\n",
    "\n",
    "Key Components of EDA:\n",
    "\n",
    "Data Summarization: Descriptive statistics like mean and median.\n",
    "Data Visualization: Visual tools (e.g., histograms, scatter plots) to identify trends.\n",
    "Handling Missing Data: Addressing incomplete data.\n",
    "Outlier Detection: Identifying extreme values.\n",
    "Correlation and Distribution: Analyzing relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Analysing all data of GEC***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_files(files):\n",
    "    for file in files:\n",
    "        try:\n",
    "            # Try reading the file with 'utf-8' encoding, fallback to 'ISO-8859-1' if error occurs\n",
    "            df = pd.read_csv(file, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(file, encoding='ISO-8859-1')\n",
    "        print(f\"File: {file}\\n\")\n",
    "        \n",
    "        # Summary Info\n",
    "        print(\"Summary:\")\n",
    "        print(df.describe(), \"\\n\")\n",
    "        \n",
    "        # Data Info\n",
    "        print(\"Info:\")\n",
    "        print(df.info(), \"\\n\")\n",
    "        \n",
    "        # Columns\n",
    "        print(\"Columns:\", df.columns.tolist(), \"\\n\")\n",
    "        \n",
    "        # Numerical and Categorical Data Counts\n",
    "        num_cols = df.select_dtypes(include='number').shape[1]\n",
    "        cat_cols = df.select_dtypes(exclude='number').shape[1]\n",
    "        print(f\"Numerical Columns: {num_cols}, Categorical Columns: {cat_cols}\\n\")\n",
    "        \n",
    "        # Null and Duplicates Check\n",
    "        print(f\"Null Values:\\n{df.isnull().sum()}\\n\")\n",
    "        print(f\"Duplicate Rows: {df.duplicated().sum()}\\n\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "\n",
    "# File paths\n",
    "files = ['stores.csv', 'customers.csv', 'exchange_rates.csv', 'sales.csv', 'products.csv']\n",
    "analyze_files(files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Customer Analysis***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis based on demographics : Continent, Country, State, gender and age groups \n",
    " #Load the customers dataset\n",
    "try:\n",
    "    customer_df = pd.read_csv('customers.csv',encoding='ISO-8859-1')  # Replace 'customers.csv' with the actual file path if needed\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'customers.csv' was not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "# Check the first few rows to verify the data\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(customer_df.head(), \"\\n\")\n",
    "\n",
    "\n",
    "# Check if required columns exist in the dataset\n",
    "required_columns = ['Continent', 'Country', 'State', 'Gender', 'Age']\n",
    "missing_columns = [col for col in required_columns if col not in customer_df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Error: The dataset is missing the following required columns: {missing_columns}\")\n",
    "    exit()\n",
    "\n",
    "# Analysis based on demographics: Continent, Country, and State\n",
    "# Continent-wise count\n",
    "continent_analysis = customer_df['Continent'].value_counts().reset_index()\n",
    "continent_analysis.columns = ['Continent', 'CustomerCount']\n",
    "print(\"Continent-wise Analysis:\")\n",
    "print(continent_analysis, \"\\n\")\n",
    "\n",
    "# Country-wise count\n",
    "country_analysis = customer_df['Country'].value_counts().reset_index()\n",
    "country_analysis.columns = ['Country', 'CustomerCount']\n",
    "print(\"Country-wise Analysis:\")\n",
    "print(country_analysis, \"\\n\")\n",
    "\n",
    "# State-wise count\n",
    "state_analysis = customer_df['State'].value_counts().reset_index()\n",
    "top_20_states = customer_df['State'].value_counts().head(20).reset_index()\n",
    "top_20_states.columns = ['State20', 'CustomerCount']\n",
    "state_analysis.columns = ['State', 'CustomerCount']\n",
    "print(\"State-wise Analysis:\")\n",
    "print(state_analysis, \"\\n\")\n",
    "\n",
    "# Gender Analysis\n",
    "gender_analysis = customer_df['Gender'].value_counts().reset_index()\n",
    "gender_analysis.columns = ['Gender', 'CustomerCount']\n",
    "print(\"Gender-wise Analysis:\")\n",
    "print(gender_analysis, \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customer Visualisation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "customer_df = pd.read_csv('customers.csv', encoding='ISO-8859-1')\n",
    "\n",
    "\n",
    "# Gender-wise analysis\n",
    "sns.countplot(data=customer_df, x='Gender', palette='Set2')\n",
    "plt.title('Gender-wise Customers (2020)')\n",
    "plt.show()\n",
    "\n",
    "# Country-wise analysis (Top 10 countries)\n",
    "customer_df['Country'].value_counts().head(10).plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 10 Countries by Customers (2020)')\n",
    "plt.ylabel('Customer Count')\n",
    "plt.show()\n",
    "\n",
    "# Continent-wise analysis\n",
    "customer_df['Continent'].value_counts().plot(kind='bar', color='orange')\n",
    "plt.title('Continent-wise Customers (2020)')\n",
    "plt.ylabel('Customer Count')\n",
    "plt.show()\n",
    "\n",
    "# Top 5 states by customer count\n",
    "customer_df['State'].value_counts().head(5).plot(kind='bar', color='green')\n",
    "plt.title('Top 5 States by Customers (2020)')\n",
    "plt.ylabel('Customer Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PRODUCT VISUALISATION##\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Load cleaned data\n",
    "file_path = \"products_cleaned.csv\"  # Ensure this file exists\n",
    "prod_df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Group data by Category and Subcategory\n",
    "category_subcategory = prod_df.groupby(['Category', 'Subcategory']).size().reset_index(name='Count')\n",
    "\n",
    "# Step 2: Bar Chart using Seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    data=category_subcategory,\n",
    "    x='Category',\n",
    "    y='Count',\n",
    "    hue='Subcategory'\n",
    ")\n",
    "plt.title('Distribution of Subcategories within Categories', fontsize=16)\n",
    "plt.xlabel('Category', fontsize=12)\n",
    "plt.ylabel('Number of Products', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Subcategory', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 1: Find unique categories\n",
    "categories = prod_df['Category'].unique()\n",
    "\n",
    "# Step 2: Loop through each category and find corresponding subcategories\n",
    "for category in categories:\n",
    "    print(f\"Category: {category}\")\n",
    "    \n",
    "    # Find subcategories for the current category\n",
    "    subcategories = prod_df[prod_df['Category'] == category]['Subcategory'].unique()\n",
    "    \n",
    "    # Loop through each subcategory and print\n",
    "    for subcategory in subcategories:\n",
    "        print(f\"  - Subcategory: {subcategory}\")\n",
    "    print()  # Add a newline between categories\n",
    "\n",
    "#Top 10 Product price\n",
    "\n",
    "# Step 1: Sort the products first by 'UnitPriceUSD' and then by 'ProductName' to break ties\n",
    "#top_10_products = prod_df[['ProductKey', 'ProductName', 'UnitPriceUSD']].sort_values(\n",
    " #   by=['UnitPriceUSD', 'ProductName'], ascending=[False, True]\n",
    "#).head(10)\n",
    "\n",
    "top_5_products = prod_df[['ProductKey', 'ProductName', 'UnitPriceUSD']].sort_values(\n",
    "    by='UnitPriceUSD', ascending=False\n",
    ").drop_duplicates(subset='UnitPriceUSD').head(5)\n",
    "\n",
    "\n",
    "# Step 2: Print the top 5 products with the highest price one by one\n",
    "print(\"Top 5 Products with Highest Price:\")\n",
    "for index, row in top_5_products.iterrows():\n",
    "    print(f\"Product Key: {row['ProductKey']}, Product Name: {row['ProductName']}, Price: ${row['UnitPriceUSD']:.2f}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sales Visualisation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Load the sales data\n",
    "sales_df = pd.read_csv('sales.csv')\n",
    "\n",
    "# Convert 'Order Date' and 'Delivery Date' to datetime format\n",
    "sales_df['Order Date'] = pd.to_datetime(sales_df['Order Date'], errors='coerce')\n",
    "sales_df['Delivery Date'] = pd.to_datetime(sales_df['Delivery Date'], errors='coerce')\n",
    "\n",
    "# Distribution of 'Quantity'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(sales_df['Quantity'], kde=True, color='blue')\n",
    "plt.title('Distribution of Quantity')\n",
    "plt.xlabel('Quantity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Check for skewness and kurtosis of 'Quantity'\n",
    "quantity_skew = skew(sales_df['Quantity'].dropna())\n",
    "quantity_kurt = kurtosis(sales_df['Quantity'].dropna())\n",
    "print(f\"Skewness of Quantity: {quantity_skew}\")\n",
    "print(f\"Kurtosis of Quantity: {quantity_kurt}\")\n",
    "\n",
    "# Outlier detection using boxplot for 'Quantity'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=sales_df['Quantity'])\n",
    "plt.title('Boxplot of Quantity')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of 'Order Date' (based on year-month or day)\n",
    "sales_df['Order Year-Month'] = sales_df['Order Date'].dt.to_period('M')\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=sales_df['Order Year-Month'], color='green')\n",
    "plt.title('Distribution of Orders by Month')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Step 1: Group Data by Year-Month\n",
    "sales_df['Order Year-Month'] = sales_df['Order Date'].dt.to_period('M')  # Convert to Year-Month period\n",
    "order_counts = sales_df.groupby('Order Year-Month').size()  # Frequency of orders per month\n",
    "\n",
    "# Step 2: Analyze the Distribution\n",
    "# Compute skewness and kurtosis\n",
    "skewness_value = skew(order_counts)\n",
    "kurtosis_value = kurtosis(order_counts)\n",
    "\n",
    "# Print skewness and kurtosis\n",
    "print(f\"Skewness: {skewness_value:.2f}\")\n",
    "print(f\"Kurtosis: {kurtosis_value:.2f}\")\n",
    "\n",
    "# Interpret skewness\n",
    "if skewness_value < -0.5:\n",
    "    skewness_interpretation = \"Negative skew (left-tailed). Earlier months had more orders.\"\n",
    "elif skewness_value > 0.5:\n",
    "    skewness_interpretation = \"Positive skew (right-tailed). Later months had more orders.\"\n",
    "else:\n",
    "    skewness_interpretation = \"Approximately symmetric. Orders are evenly distributed over time.\"\n",
    "\n",
    "# Interpret kurtosis\n",
    "if kurtosis_value < 3:\n",
    "    kurtosis_interpretation = \"Platykurtic (flatter distribution).\"\n",
    "elif kurtosis_value > 3:\n",
    "    kurtosis_interpretation = \"Leptokurtic (sharper peak distribution).\"\n",
    "else:\n",
    "    kurtosis_interpretation = \"Mesokurtic (similar to a normal distribution).\"\n",
    "\n",
    "print(f\"Skewness Interpretation: {skewness_interpretation}\")\n",
    "print(f\"Kurtosis Interpretation: {kurtosis_interpretation}\")\n",
    "\n",
    "# Step 3: Plot the Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=order_counts.index.astype(str), y=order_counts.values, color='green')\n",
    "plt.title('Distribution of Orders by Month')\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Summarize Inference\n",
    "if skewness_value > 0:\n",
    "    print(\"Sales appear to increase over time, as later months have higher frequency of orders.\")\n",
    "elif skewness_value < 0:\n",
    "    print(\"Sales appear to decrease over time, as earlier months have higher frequency of orders.\")\n",
    "else:\n",
    "    print(\"Sales are consistent over time, with no significant trend.\")\n",
    "\n",
    "\n",
    "# Distribution of 'Currency Code'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=sales_df['Currency Code'], palette='Set2')\n",
    "plt.title('Distribution of Currency Codes')\n",
    "plt.xlabel('Currency Code')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of 'StoreKey'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=sales_df['StoreKey'], palette='Set3')\n",
    "plt.title('Distribution of Sales by Store')\n",
    "plt.xlabel('Store Key')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of 'ProductKey' (if needed, top N products)\n",
    "top_products = sales_df['ProductKey'].value_counts().nlargest(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_products.plot(kind='bar', color='purple')\n",
    "plt.title('Top 10 Products by Sales Frequency')\n",
    "plt.xlabel('Product Key')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Program to compare the revenue distributions for two years \n",
    "# from the merged_sales.csv file using SciPy and Matplotlib.\n",
    "\n",
    "# Steps:\n",
    "# 1. Load the dataset using pandas.\n",
    "# 2. Convert the 'Order Date' column to a datetime format and extract the 'Year'.\n",
    "# 3. Filter and group data by 'Order Date' to calculate daily revenue for two specific years.\n",
    "# 4. Use SciPy's gaussian_kde to calculate the kernel density estimate (KDE) for both years.\n",
    "# 5. Define a range of revenue values (x-axis) to plot the KDE for both years on the same graph.\n",
    "# 6. Plot the distributions using matplotlib with different colors and add a shaded area \n",
    "#    to highlight the overlap between the distributions.\n",
    "# 7. Customize the plot with titles, labels, a legend, and a grid for better visualization.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv(\"merged_sales.csv\")\n",
    "\n",
    "# Step 2: Ensure 'Order Date' is in datetime format and extract the year\n",
    "data['Order Date'] = pd.to_datetime(data['Order Date'])\n",
    "data['Year'] = data['Order Date'].dt.year\n",
    "\n",
    "# Step 3: Group data by year and calculate yearly revenue\n",
    "yearly_revenue = data.groupby(['Year'])['Revenue'].sum().reset_index()\n",
    "\n",
    "# Select two years to compare (e.g., 2022 and 2023)\n",
    "year1 = 2020\n",
    "year2 = 2019\n",
    "\n",
    "# Filter data for the two years\n",
    "data_year1 = data[data['Year'] == year1]\n",
    "data_year2 = data[data['Year'] == year2]\n",
    "\n",
    "# Step 4: Calculate revenue distribution for the selected years\n",
    "# Group by 'Order Date' to calculate daily revenue for both years\n",
    "revenue_year1 = data_year1.groupby('Order Date')['Revenue'].sum().values\n",
    "revenue_year2 = data_year2.groupby('Order Date')['Revenue'].sum().values\n",
    "\n",
    "# Step 5: Use SciPy's gaussian_kde to calculate kernel density estimates for each year\n",
    "kde_year1 = gaussian_kde(revenue_year1)\n",
    "kde_year2 = gaussian_kde(revenue_year2)\n",
    "\n",
    "# Create a range for the x-axis based on the minimum and maximum revenue values across both years\n",
    "x_range = np.linspace(min(revenue_year1.min(), revenue_year2.min()),\n",
    "                      max(revenue_year1.max(), revenue_year2.max()), 100)\n",
    "\n",
    "# Step 6: Plot the distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot KDE for Year 1\n",
    "plt.plot(x_range, kde_year1(x_range), label=f\"Year {year1} Revenue Distribution\", color='blue')\n",
    "\n",
    "# Plot KDE for Year 2\n",
    "plt.plot(x_range, kde_year2(x_range), label=f\"Year {year2} Revenue Distribution\", color='orange')\n",
    "\n",
    "# Highlight the overlap between the distributions\n",
    "plt.fill_between(x_range, kde_year1(x_range), kde_year2(x_range), color='gray', alpha=0.3, label=\"Overlap Area\")\n",
    "\n",
    "# Step 7: Customize the plot\n",
    "plt.title(\"Revenue Distribution Comparison\")\n",
    "plt.xlabel(\"Revenue\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALISATION INFERENCE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### North America has large number of customers.\n",
    "##### Male and Female distribution is almost Same.\n",
    "##### California city has large number of customers and in countries, US has broader customers.\n",
    "##### The highest selling price of the product is $2899.99. \n",
    "##### In Categories, Computers has seen much revenue over the period of 2016-2021.\n",
    "##### Subcategries under each Category which has a market trend is studied.\n",
    "##### Top 10 products has a uniform distibution to infer the revenue yielded is almost same.\n",
    "##### The distribution of product sales over month from 2016-21 says that there is hike in revene along the years(Positive Skewness)\n",
    "##### Store 0, which is online mode has made remarkarble spike in heights. This makes the company make informed decsion on its mark of presence.\n",
    "##### USD, has the highest transactions in terms of currency.\n",
    "##### Economically, year 2020 has seen progress compared to the previos year 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SCHEMA IN MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "from datetime import datetime\n",
    "\n",
    "# Load dataset with error handling for encoding issues\n",
    "file_path = \"customers.csv\"  # Replace with the correct path\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='latin1')  # Handle UnicodeDecodeError\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Remove duplicates based on 'CustomerKey'\n",
    "if df.duplicated(subset=['CustomerKey']).sum() > 0:\n",
    "    print(f\"Found {df.duplicated(subset=['CustomerKey']).sum()} duplicates. Removing them...\")\n",
    "    df = df.drop_duplicates(subset=['CustomerKey'])\n",
    "\n",
    "# Convert the 'Birthday' column to MySQL-compatible format (YYYY-MM-DD)\n",
    "if 'Birthday' in df.columns:\n",
    "    def convert_to_mysql_date(date):\n",
    "        try:\n",
    "            return pd.to_datetime(date, errors='coerce').strftime('%Y-%m-%d')  # Coerce invalid dates to NaT\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    df['Birthday'] = df['Birthday'].apply(convert_to_mysql_date)\n",
    "    \n",
    "    # Remove rows with invalid 'Birthday' values (NaT)\n",
    "    df = df.dropna(subset=['Birthday'])\n",
    "\n",
    "# Ensure no NaN values before inserting into the database\n",
    "df = df.dropna()\n",
    "df.to_csv('customers_cleaned.csv',index=False)\n",
    "\n",
    "# Database connection details\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = 'root'\n",
    "database = 'gelect'\n",
    "\n",
    "# Establish MySQL connection using pymysql\n",
    "try:\n",
    "    conn = pymysql.connect(host=host, user=user, password=password, database=database)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create Customers table if it doesn't exist\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Customers (\n",
    "        CustomerKey INT PRIMARY KEY,\n",
    "        Gender VARCHAR(10),\n",
    "        Name VARCHAR(100),\n",
    "        City VARCHAR(100),\n",
    "        StateCode VARCHAR(10),\n",
    "        State VARCHAR(100),\n",
    "        ZipCode VARCHAR(10),\n",
    "        Country VARCHAR(50),\n",
    "        Continent VARCHAR(50),\n",
    "        Birthday DATE\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    print(\"Customers table created or already exists.\")\n",
    "\n",
    "    # Insert data into Customers table\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO Customers (CustomerKey, Gender, Name, City, StateCode, State, ZipCode, Country, Continent, Birthday)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    ON DUPLICATE KEY UPDATE\n",
    "        Gender=VALUES(Gender),\n",
    "        Name=VALUES(Name),\n",
    "        City=VALUES(City),\n",
    "        StateCode=VALUES(StateCode),\n",
    "        State=VALUES(State),\n",
    "        ZipCode=VALUES(ZipCode),\n",
    "        Country=VALUES(Country),\n",
    "        Continent=VALUES(Continent),\n",
    "        Birthday=VALUES(Birthday);\n",
    "    \"\"\"\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "    conn.commit()\n",
    "    print(\"Data inserted successfully!\")\n",
    "\n",
    "except pymysql.MySQLError as e:\n",
    "    print(f\"MySQL Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'conn' in locals() and conn.open:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# The above code ensures removal of duplicates customer Key(ID) and the nan and empty values of birthday is addressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#product Insertion\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = \"products.csv\"  # Replace with your actual file path\n",
    "prod_df = pd.read_csv(file_path, encoding='latin1')  # Handle encoding issues\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Step 2: Clean the dataset\n",
    "prod_df.columns = prod_df.columns.str.strip()\n",
    "column_mapping = {\n",
    "    \"Product Key\": \"ProductKey\",\n",
    "    \"Product Name\": \"ProductName\",\n",
    "    \"Unit Cost USD\": \"UnitCostUSD\",\n",
    "    \"Unit Price USD\": \"UnitPriceUSD\",\n",
    "    \"Subcategory Key\": \"SubcategoryKey\",\n",
    "    \"Category Key\": \"CategoryKey\"\n",
    "}\n",
    "prod_df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "prod_df['UnitCostUSD'] = prod_df['UnitCostUSD'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "prod_df['UnitPriceUSD'] = prod_df['UnitPriceUSD'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "prod_df.drop_duplicates(subset=['ProductKey'], inplace=True)\n",
    "\n",
    "# Check for missing ProductKey values\n",
    "if prod_df['ProductKey'].isnull().any():\n",
    "    print(\"Missing ProductKey values detected. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Print total rows and unique ProductKeys\n",
    "unique_keys = prod_df['ProductKey'].nunique()\n",
    "total_rows = prod_df.shape[0]\n",
    "print(f\"Total Rows: {total_rows}, Unique Product Keys: {unique_keys}\")\n",
    "\n",
    "# Step 3: Connect to MySQL\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = 'root'\n",
    "database = 'gelect'\n",
    "\n",
    "conn = pymysql.connect(host=host, user=user, password=password, database=database)\n",
    "cursor = conn.cursor()\n",
    "print(\"Connected to the database!\")\n",
    "\n",
    "# Create Products table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Products (\n",
    "    ProductKey INT PRIMARY KEY,\n",
    "    ProductName VARCHAR(255),\n",
    "    Brand VARCHAR(100),\n",
    "    Color VARCHAR(50),\n",
    "    UnitCostUSD DECIMAL(10,2),\n",
    "    UnitPriceUSD DECIMAL(10,2),\n",
    "    SubcategoryKey INT,\n",
    "    Subcategory VARCHAR(100),\n",
    "    CategoryKey INT,\n",
    "    Category VARCHAR(100)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# Insert data\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO Products (ProductKey, ProductName, Brand, Color, UnitCostUSD, UnitPriceUSD, SubcategoryKey, Subcategory, CategoryKey, Category)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "ON DUPLICATE KEY UPDATE\n",
    "    ProductName=VALUES(ProductName),\n",
    "    Brand=VALUES(Brand),\n",
    "    Color=VALUES(Color),\n",
    "    UnitCostUSD=VALUES(UnitCostUSD),\n",
    "    UnitPriceUSD=VALUES(UnitPriceUSD),\n",
    "    SubcategoryKey=VALUES(SubcategoryKey),\n",
    "    Subcategory=VALUES(Subcategory),\n",
    "    CategoryKey=VALUES(CategoryKey),\n",
    "    Category=VALUES(Category);\n",
    "\"\"\"\n",
    "\n",
    "# Debugging: Track rows successfully inserted\n",
    "inserted_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for _, row in prod_df.iterrows():\n",
    "    try:\n",
    "        cursor.execute(insert_query, tuple(row))\n",
    "        inserted_count += 1\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(f\"Error inserting row with ProductKey {row['ProductKey']}: {e}\")\n",
    "        skipped_count += 1\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "print(f\"Rows inserted: {inserted_count}\")\n",
    "print(f\"Rows skipped: {skipped_count}\")\n",
    "\n",
    "# Compare row counts\n",
    "cursor.execute(\"SELECT COUNT(*) FROM Products;\")\n",
    "db_row_count = cursor.fetchone()[0]\n",
    "print(f\"Rows in database: {db_row_count}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Database connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SALES INSERTION\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_date(date_str):\n",
    "    \"\"\"\n",
    "    Converts a date string to the MySQL-compatible format (YYYY-MM-DD).\n",
    "    Returns None if the date is invalid.\n",
    "    \"\"\"\n",
    "    if pd.isnull(date_str):\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        print(f\"Invalid date format for {date_str}\")\n",
    "        return None\n",
    "\n",
    "def insert_data(cursor, insert_query, data):\n",
    "    \"\"\"\n",
    "    Inserts a single row of data into the database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor.execute(insert_query, data)\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(f\"Error inserting row: {e}\")\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the sales data by removing unnecessary columns, handling missing values,\n",
    "    and converting date columns.\n",
    "    \"\"\"\n",
    "    # Drop unnecessary columns (e.g., 'Unnamed: 0')\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Convert date columns to MySQL-compatible format\n",
    "    df['Order Date'] = df['Order Date'].apply(convert_date)\n",
    "    df['Delivery Date'] = df['Delivery Date'].apply(convert_date)\n",
    "\n",
    "    # Remove rows with invalid dates\n",
    "    df.dropna(subset=['Order Date', 'Delivery Date'], inplace=True)\n",
    "\n",
    "    # Replace empty strings with None\n",
    "    df.replace(r'^\\s*$', None, regex=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the sales.csv file\n",
    "sales_df = pd.read_csv('sales.csv')\n",
    "\n",
    "# Clean the data\n",
    "sales_cleaned_df = clean_data(sales_df)\n",
    "\n",
    "# Debugging: Print the first few rows of cleaned data\n",
    "print(\"Cleaned Data:\")\n",
    "print(sales_cleaned_df.head())\n",
    "\n",
    "# Connect to MySQL\n",
    "conn = pymysql.connect(host='localhost', user='root', password='root', db='gelect')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the Sales table if it does not exist\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Sales (\n",
    "    `Order Number` BIGINT,  -- Changed to INT64 equivalent\n",
    "    `Line Item` INT,\n",
    "    `Order Date` DATE,\n",
    "    `Delivery Date` DATE,\n",
    "    `CustomerKey` INT,\n",
    "    `StoreKey` INT,\n",
    "    `ProductKey` INT,\n",
    "    `Quantity` INT,\n",
    "    `Currency Code` VARCHAR(10)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# Insert data into the Sales table\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO Sales (`Order Number`, `Line Item`, `Order Date`, `Delivery Date`, `CustomerKey`, `StoreKey`, `ProductKey`, `Quantity`, `Currency Code`)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Debugging: Print the number of rows to be inserted\n",
    "print(f\"Total rows to insert: {len(sales_cleaned_df)}\")\n",
    "\n",
    "# Loop through the DataFrame rows and insert them into the database\n",
    "inserted_count = 0\n",
    "for _, row in sales_cleaned_df.iterrows():\n",
    "    # Convert the row into a tuple and replace NaN with None\n",
    "    data = tuple(row)\n",
    "\n",
    "    # Debugging: Print the data being inserted\n",
    "    print(f\"Inserting row: {data}\")\n",
    "\n",
    "    # Check if the data tuple has 9 elements and does not contain None\n",
    "    if len(data) == 9 and all(element is not None for element in data):  # Allow 0 as a valid value\n",
    "        insert_data(cursor, insert_query, data)\n",
    "        inserted_count += 1\n",
    "    else:\n",
    "        print(f\"Skipping row due to invalid or missing data: {data}\")\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Confirm successful insertion\n",
    "print(f\"Data inserted successfully into the Sales table! Total rows inserted: {inserted_count}\")\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stores Insertion\n",
    "#Store Analysis and Data\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to convert date to MySQL format\n",
    "def convert_date(date_str):\n",
    "    if pd.isnull(date_str):\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        print(f\"Invalid date format for {date_str}\")\n",
    "        return None\n",
    "\n",
    "# Load the stores.csv file\n",
    "stores_df = pd.read_csv('stores.csv')\n",
    "\n",
    "# Clean the data\n",
    "stores_df['Open Date'] = stores_df['Open Date'].apply(convert_date)  # Convert Open Date to MySQL format\n",
    "stores_df.dropna(inplace=True)  # Drop rows with missing values\n",
    "\n",
    "# Debug: Print the first few rows of the cleaned data\n",
    "print(\"Cleaned Stores Data:\")\n",
    "print(stores_df.head())\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = pymysql.connect(host='localhost', user='root', password='root', db='gelect')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the Stores table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Stores (\n",
    "    StoreKey INT PRIMARY KEY,\n",
    "    Country VARCHAR(100),\n",
    "    State VARCHAR(100),\n",
    "    SquareMeters INT,\n",
    "    OpenDate DATE\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "print(\"Stores table created successfully!\")\n",
    "\n",
    "# Insert data into the Stores table\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO Stores (StoreKey, Country, State, SquareMeters, OpenDate)\n",
    "VALUES (%s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "# Loop through the DataFrame rows and insert data\n",
    "for _, row in stores_df.iterrows():\n",
    "    data = (row['StoreKey'], row['Country'], row['State'], row['Square Meters'], row['Open Date'])\n",
    "    try:\n",
    "        cursor.execute(insert_query, data)\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(f\"Error inserting row {data}: {e}\")\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Confirm success\n",
    "print(f\"Data inserted successfully into the Stores table! Total rows inserted: {cursor.rowcount}\")\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exchange Rate Insertion\n",
    "import pymysql\n",
    "import csv\n",
    "\n",
    "# Database connection details\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = 'root'\n",
    "database = 'gelect'\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = 'exchange_rates.csv'\n",
    "\n",
    "try:\n",
    "    # Connect to the database\n",
    "    connection = pymysql.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        database=database\n",
    "    )\n",
    "    print(\"Connected to the database successfully.\")\n",
    "    \n",
    "    # Create a cursor object\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # SQL query to create the table\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS exchange_rate (\n",
    "        date VARCHAR(20),\n",
    "        currency VARCHAR(20),\n",
    "        exchange_rate DECIMAL(10, 5)\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query to create the table\n",
    "    cursor.execute(create_table_query)\n",
    "    print(\"Table 'exchange_rate' created successfully.\")\n",
    "    \n",
    "    # Read the CSV file and insert data into the table\n",
    "    with open(csv_file_path, mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader)  # Skip the header row if the CSV file has one\n",
    "        \n",
    "        # Insert data row by row\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO exchange_rate (date, currency, exchange_rate)\n",
    "        VALUES (%s, %s, %s);\n",
    "        \"\"\"\n",
    "        for row in csv_reader:\n",
    "            cursor.execute(insert_query, row)\n",
    "        \n",
    "        # Commit the changes\n",
    "        connection.commit()\n",
    "        print(\"Data inserted into 'exchange_rate' table successfully.\")\n",
    "    \n",
    "except pymysql.MySQLError as e:\n",
    "    print(\"Error connecting to the database or inserting data:\", e)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: CSV file '{csv_file_path}' not found.\")\n",
    "    \n",
    "finally:\n",
    "    # Close the connection\n",
    "    if connection:\n",
    "        connection.close()\n",
    "        print(\"Database connection closed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MERGING DATA TO FORM RELATIONSHIP AND TO FILTER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging Sales, customers, products(exchangerates and stores are subsequntly merged as per need) for filters\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets (replace with your actual file paths)\n",
    "sales_df = pd.read_csv('sales_cleaned.csv')  # Replace with your actual file path\n",
    "products_df = pd.read_csv('products_cleaned.csv')  # Replace with your actual file path\n",
    "stores_df = pd.read_csv('stores_cleaned.csv')  # Replace with your actual file path\n",
    "\n",
    "# Merge sales with products data\n",
    "sales_products_df = pd.merge(sales_df, products_df, on='ProductKey', how='inner')\n",
    "\n",
    "# Merge sales-products with stores data\n",
    "merged_sales_df = pd.merge(sales_products_df, stores_df, on='StoreKey', how='inner')\n",
    "\n",
    "# Calculate the revenue for each sale (Unit Price * Quantity)\n",
    "merged_sales_df['Revenue'] = merged_sales_df['UnitPriceUSD'] * merged_sales_df['Quantity']\n",
    "\n",
    "# Check the resulting merged data\n",
    "print(\"\\nMerged Sales Data:\")\n",
    "print(merged_sales_df.head())\n",
    "print(merged_sales_df.info())\n",
    "\n",
    "# Check for missing values in merged data\n",
    "print(\"\\nMissing values in merged data:\")\n",
    "print(merged_sales_df.isnull().sum())\n",
    "\n",
    "# write the both merged files to csv\n",
    "# Write the sales_products_df to a CSV file\n",
    "sales_products_df.to_csv('sales_products.csv', index=False)\n",
    "\n",
    "# Write the merged_sales_df to a CSV file\n",
    "merged_sales_df.to_csv('merged_sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "#### The DPD (Data Powered Decision) Project for Global Electronics and Communications utilized Pandas, Matplotlib, PyMySQL, and Power BI. EDA and visualizations were performed to identify trends and anomalies, while Power BI dashboards provided deeper insights. A structured data schema was also developed, enabling data-driven decision-making to optimize operations and improve customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
